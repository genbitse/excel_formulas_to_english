#!/usr/bin/env python

__author__ = "Martin Rydén"
__copyright__ = "Copyright 2016, Martin Rydén"
__license__ = "MIT"
__version__ = "1.0.1"
__email__ = "pemryd@gmail.com"

import re
import requests
from bs4 import BeautifulSoup
from GoogleScraper import scrape_with_config, GoogleSearchError
import collections

# Input Excel formula
#formula = input("\nFunction: ")
formula = '=INDEX(A1:Z10,MATCH("product",A1:A10,0),MATCH("month",A1:Z1,0))'

# Split by and keep any non-alphanumeric delimiter, filter blanks
dlformula = list(filter(None, re.split('([^\\w.\"\:])', formula)))


# List of functions
functions = []

with open('excel_functions.txt') as f:
    lines = f.read().splitlines()
    for fc in dlformula:
        for f in lines:
            if(fc == f):
                functions.append(fc)

# =INDEX(A1:Z10,MATCH("product",A1:A10,0),MATCH("month",A1:Z1,0))

# Set regex pattern: any alpha-numeric characters
pattern = re.compile('([\w\.\"\:]+)',re.I)
#wc = [(pattern.sub("*", f)) for f in nfunction if f not in const]

#print(re1_matches)

wc = []
tfunction = []

for f in dlformula:
    if f in functions:
        wc.append(f)
        tfunction.append(f)
    else:
        a = pattern.sub("*", f)
        wc.append(a)

wcf = ''.join(wc)+"*"

print(wcf)

"""
### Test ###

tempurl = "http://www.deskbright.com/excel/using-index-match-match/"

tm = '=INDEX(B2:D7, MATCH(G2, B2:B7, 0), MATCH(G3, B2:D2, 0)Output: 456'

###########


r = requests.get(tempurl)
soup = BeautifulSoup(r.content, "html.parser")

matches = collections.defaultdict(int)

def find_elements(el):    
    for p in (soup.find_all(el)):
        if(all(x in p.getText() for x in tfunction)):
            matches[el] += 1 
            #print(p.getText())

elements = ['pre', 'p', 'ul']         
for e in elements:
    find_elements(e)
"""

#### Scrape google for top hits ####

config = {
    'use_own_ip': 'True',
    'keyword': ("%s excel formula -youtube") % wcf,
    'search_engines': ['google'],
    'num_pages_for_keyword': 1,
    'scrape_method': 'http',
    'do_caching': 'True',
    'print_results': 'summarize'
}

urls = [] # List of result URLs

try:
    search = scrape_with_config(config)
except GoogleSearchError as e:
    pass

# Results

for serp in search.serps:
    for link in serp.links:
        urls.append(link.link)
        
#### Parse into BS4 ####



ranking = collections.defaultdict(dict)
sub = collections.defaultdict(dict)

def find_elements(el):    
        for p in (soup.find_all(el)):
            if(all(x in p.getText() for x in tfunction)):
                matches[el] += 1
                #print(p.getText())

webid = 0

for url in urls:
    webid += 1
    matches = collections.defaultdict(int)    
    
    r = requests.get(url)
    soup = BeautifulSoup(r.content, "html.parser")
    elements = ['pre', 'p', 'ul']         
    for e in elements:
        find_elements(e)
    
    try:
        stitle = soup.title.string
    except:
        stitle = "No title"
 
    print('\nFound matches in "%s".\n \
    URL: %s' % (stitle,url))
    
    ranking[webid] = #(matches.items(),url)
    
    #for k,v in matches.items():
    #    print(k,v)
    
for x in ranking.values():
    try:
        for k,v in x.items():
            print(k,v)        
    except:
        pass


####


"""

### TEMP TEMP TEMP TEMP

for fc in spfunction:
    for f in all_functions:
        if(fc == f):
            print(fc)
    


# Find all tables in selected languages
url = "http://www.piuha.fi/excel-function-name-translation/index.php?page=%s-%s.html" % (avail_lang[langf][0], avail_lang[langt][1])
r = requests.get(url)
soup = BeautifulSoup(r.content, "html.parser")

# Create dict and enumerate over table values
tdict = dict((i,t) for i,t in enumerate(soup.find_all('td'))) 

# Get rid of '=' for now
function = function.replace('=','')

# Split by and keep any non-alphanumeric delimiter (except .)
# the full string including delimters is added at the end
spfunction = re.split('([^\\w.])', function)

# Split by and remove delimiters, filter out empty elements
function = list(filter(None, re.split(r'[\W]+', function)))

# Only keep elements longer than 2 chars, in order to limit matching
function = [f for f in function if len(f) > 2]

# Iterate over table values, function parts
# Add original and translated values to dict
trdict = {}
#for i, t in tdict.items():
for i, t in tdict.items():
    for x in function:
         if(str(x) in str(t.getText())):
              if(langf == "en"):
                   if(t.getText() not in trdict.keys()): # Prevent duplicates
                       fr = (tdict[i+1].getText().split(','))[0]
                       to = (t.getText().split(','))[0]
                       trdict[to] = fr # If translating from English, set key to English
              else: 
                   fr = (t.getText().split(','))[0]
                   to = (tdict[i+1].getText().split(','))[0]
                   trdict[fr] = to # If translating from non-English, set key to non-English


new = [trdict.get(x,x) for x in spfunction] # Get translated value from dict

if(langf != "en"):
     new = '=' + ''.join(new).replace(';',',')
elif(langf == "en"):
     new = '=' + ''.join(new).replace(',',';')

print("\n%s"%new)
"""
